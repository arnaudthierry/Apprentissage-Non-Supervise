# Algorithme K-means : Initialisation et Optima Locaux

![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/11e27de6-5ded-438f-a73a-af612c03909f)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/ecccc46b-8aae-471b-b811-bccf8dbd42b1)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/c69cf31e-77bc-4ebe-80bf-3db621a66e1d)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/7d95365d-b7f9-4c4b-b353-cbbccd7a6f93)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/1e445e0d-725c-45df-8a5a-153ad833edc3)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/a9d1096d-18bf-4879-8b60-dca66050b2f5)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/61a2a6f4-af03-4722-b924-3e15ae61c955)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/42f8cd54-1fb2-4b0e-9a20-6fb0c20ab1e7)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/8be27d17-457d-471a-8ddd-6b78add52ef8)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/5627ca66-88e6-4e3f-893e-f5239a6c587b)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/cb163fdb-9e3a-433f-83fa-1aa039e6be19)











# 1 -  Étape d'Initialisation

La première étape de l'algorithme de regroupement K-means consiste à choisir des emplacements aléatoires comme estimations initiales pour les centroïdes de cluster, notés $$\mu_1$$ à $$\mu_K$$. Cette estimation aléatoire est généralement réalisée en sélectionnant aléatoirement K exemples d'entraînement dans notre ensemble de données.

# 2 - Importance des Initialisations Multiples

- Lors de l'exécution de l'algorithme K-means, il est courant de choisir un nombre K de clusters inférieur au nombre d'exemples d'entraînement m. Cela est dû au fait que si K est supérieur à m, il n'y aurait pas assez d'exemples d'entraînement pour avoir au moins un exemple par centroïde de cluster.
- Cependant, avec une initialisation aléatoire des centroïdes, l'algorithme peut se retrouver coincé dans des optima locaux, ce qui peut conduire à des résultats de regroupement moins satisfaisants.

# 3 - Illustration des Optima Locaux

Prenons un exemple où nous essayons de trouver trois clusters (K = 3) dans un ensemble de données. Avec une initialisation aléatoire, l'algorithme K-means pourrait obtenir un regroupement qui semble correct. Cependant, une autre initialisation pourrait donner un résultat sous-optimal. Cette variabilité est due au fait que K-means tente de minimiser une fonction de coût de distorsion, notée $$J$$, en fonction des clusters.

Pour éviter de se retrouver dans un minimum local, une technique courante consiste à exécuter l'algorithme K-means plusieurs fois avec des initialisations aléatoires différentes. Ensuite, vous choisissez le regroupement qui a le coût $$J$$ *le plus faible.*

# 4 - Méthode de Sélection des Initialisations

Voici l'algorithme pour utiliser des initialisations multiples :

1. Choisissez K exemples d'entraînement au hasard.
2. Utilisez ces exemples pour initialiser les centroïdes des clusters.
3. Exécutez l'algorithme K-means jusqu'à convergence.
4. Calculez la fonction de coût $$J$$.
5. Répétez les étapes 1 à 4 pour un grand nombre de fois (50 à 1000 fois est courant).
6. Sélectionnez le regroupement avec le coût $$J$$ *le plus bas.*


# Pseudo-code de l'algorithme  :


## A - Pour \(i = 1\) à \(N\) (nombre d'initialisations) :
1. Choisissez \(K\) exemples d'entraînement au hasard.
2. Initialisez les centroïdes des clusters à ces \(K\) exemples.
3. Exécutez K-means jusqu'à convergence pour obtenir les centroïdes et les attributions de clusters.
4. Calculez la distorsion et la fonction de coût \(J\).

## B - Sélectionnez le regroupement avec le coût \(J\) le plus bas parmi les \(N\) initialisations.


# 5-  Conclusion

- En utilisant cette technique, vous augmentez vos chances de trouver le meilleur regroupement possible.
- Même si exécuter K-means plusieurs fois peut être coûteux en termes de calcul, les résultats seront généralement bien meilleurs qu'avec une seule initialisation.
- Avant de terminer notre discussion sur K-means, nous devons encore aborder la question du choix du nombre de clusters \(K\).
- Passons à la prochaine section pour explorer comment déterminer la valeur optimale de \(K\).


### Équation de Mise à Jour du Centroïde

L'algorithme K-means utilise la formule suivante pour mettre à jour les centroïdes des clusters :
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/038b4417-66b7-4e4d-bbee-32223c6b34dd)

où :
- $$\boldsymbol{\mu}_k$$ est le centroïde du cluster $$C_k$$.
- $$|C_k|$$ est le nombre de points dans le cluster $$C_k$$.
- $$\mathbf{x}$$ représente les points assignés au cluster $$C_k$$.

- Cette formule signifie que nous calculons la moyenne des coordonnées de tous les points dans le cluster $$C_k$$ pour obtenir la nouvelle position du centroïde.


# 6 - Annexe 1 - Résumé sur l' algorithme K-means : Initialisation et Optima Locaux

- L'algorithme K-means est une méthode de partitionnement de données largement utilisée pour regrouper un ensemble de données en K clusters. 
- L'algorithme K-means fonctionne de manière itérative pour minimiser la somme des distances entre chaque point de données et le centroïde de son cluster.
- Le processus général est le suivant :

1. **Choisir K** : Déterminer le nombre de clusters K.
2. **Initialiser les centroïdes** : Sélectionner aléatoirement K points de données comme centroïdes initiaux.
3. **Attribuer les points aux clusters** : Assigner chaque point de données au cluster dont le centroïde est le plus proche.
4. **Mettre à jour les centroïdes** : Recalculer les centroïdes en prenant la moyenne des points de chaque cluster.
5. **Répéter** : Répéter les étapes 3 et 4 jusqu'à ce que les centroïdes ne changent plus (convergence).

## 6.1. Importance de l'Initialisation des Centroïdes

L'initialisation des centroïdes est cruciale car elle influence fortement la convergence et les résultats finaux de l'algorithme. Une mauvaise initialisation peut conduire à des optima locaux indésirables, où l'algorithme se bloque dans une solution sous-optimale.

##### Méthodes d'Initialisation

1. **Initialisation Aléatoire** : Choisir K points de données au hasard comme centroïdes initiaux. Cette méthode est simple mais peut conduire à des résultats variés et suboptimaux.
2. **K-means++** : Sélectionner les centroïdes initiaux de manière à maximiser la distance entre eux, ce qui améliore souvent la convergence et la qualité des clusters[5].

## 6.2. Optima Locaux

L'algorithme K-means peut se bloquer dans des optima locaux, où il trouve une solution qui n'est pas globalement optimale mais ne peut pas être améliorée par des ajustements locaux.

##### Exemples d'Optima Locaux

- **Initialisation Suboptimale** : Si les centroïdes initiaux sont mal choisis, l'algorithme peut converger vers une solution où les clusters ne sont pas bien séparés[1][3].
- **Clusters Mal Formés** : Des initialisations différentes peuvent produire des clusters de qualité variable, certains étant clairement meilleurs que d'autres[1].

## 6.3. Stratégies pour Éviter les Optima Locaux

1. **Multiples Initialisations** : Exécuter l'algorithme plusieurs fois avec des initialisations différentes et choisir la solution avec le coût le plus bas (fonction de distorsion J)[1][3].
2. **Méthode du Coude** : Utiliser des techniques comme la méthode du coude pour déterminer le nombre optimal de clusters, ce qui peut aider à éviter des configurations suboptimales[2][4].

## 6.4. Conclusion

L'algorithme K-means est puissant mais sensible à l'initialisation des centroïdes et aux optima locaux. En utilisant des techniques comme K-means++ et en exécutant l'algorithme plusieurs fois avec des initialisations différentes, on peut améliorer la qualité des clusters obtenus.

## 6.5. Implémentation pratique en Python et la librairie Scikit-learn :

```python
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Générer des données aléatoires
X = np.random.rand(100, 2)

# Initialiser et ajuster le modèle K-means
kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10)
kmeans.fit(X)

# Afficher les résultats
plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')
plt.show()
```

- En suivant ces principes, vous pouvez utiliser l'algorithme K-means de manière plus efficace et obtenir des résultats de clustering de meilleure qualité.

Citations:
- [1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/19265956/746ab27d-182c-4f13-a136-8aead9a448e6/paste.txt
- [2] https://www.intelligence-artificielle-school.com/ecole/technologies/k-means-algorithme/
- [3] https://mrmint.fr/algorithme-k-means
- [4] https://www.jedha.co/formation-ia/algorithme-kmeans
- [5] https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html
- [6] https://fr.wikipedia.org/wiki/K-moyennes
- [7] https://en.wikipedia.org/wiki/K-means_clustering
- [8] https://pubmed.ncbi.nlm.nih.gov/16784337/
- [9] https://catalyst.earth/catalyst-system-files/help/concepts/focus_c/oa_classif_intro_unsuperClass_kmns.html
- [10] https://automaticaddison.com/k-means-clustering-and-the-local-search-problem/
