## Objectif d'optimisation de l'algorithme K-means

L'algorithme K-means vise à minimiser une fonction de coût appelée "distorsion" ou "erreur de quantification". Cette fonction mesure la distance moyenne au carré entre chaque point de données et le centroïde du cluster auquel il est assigné.

# 1 - Définition de la fonction de coût

Soit $\mathbf{X} = \{\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_N\}$ l'ensemble des $N$ points de données dans $\mathbb{R}^d$. 

Soit $\mathcal{C} = \{C_1, C_2, ..., C_K\}$ une partition de $\mathbf{X}$ en $K$ clusters.

Soit $\boldsymbol{\mu} = \{\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, ..., \boldsymbol{\mu}_K\}$ l'ensemble des $K$ centroïdes, où $\boldsymbol{\mu}_k$ est le centroïde du cluster $C_k$.

La fonction de coût à minimiser est définie comme :

$$J(\mathcal{C}, \boldsymbol{\mu}) = \sum_{k=1}^{K} \sum_{\mathbf{x} \in C_k} \|\mathbf{x} - \boldsymbol{\mu}_k\|^2$$

Où $\|\mathbf{x} - \boldsymbol{\mu}_k\|^2$ est la distance euclidienne au carré entre le point $\mathbf{x}$ et le centroïde $\boldsymbol{\mu}_k$ du cluster $C_k$ auquel il est assigné.

# 2 - Minimisation de la fonction de coût

L'algorithme K-means minimise cette fonction de coût en deux étapes alternées :

1. **Étape d'assignation** : Pour des centroïdes $\boldsymbol{\mu}$ fixés, assigner chaque point $\mathbf{x}_i$ au cluster $C_k$ dont le centroïde $\boldsymbol{\mu}_k$ est le plus proche, i.e. minimiser $\|\mathbf{x}_i - \boldsymbol{\mu}_k\|^2$.

2. **Étape de mise à jour des centroïdes** : Pour une assignation $\mathcal{C}$ fixée, mettre à jour chaque centroïde $\boldsymbol{\mu}_k$ comme la moyenne des points assignés à $C_k$, i.e. $\boldsymbol{\mu}_k = \frac{1}{|C_k|} \sum_{\mathbf{x} \in C_k} \mathbf{x}$.

Ces deux étapes sont répétées alternativement jusqu'à convergence vers un minimum local de $J(\mathcal{C}, \boldsymbol{\mu})$.

# 3 - Propriétés de convergence

- À chaque itération, la fonction de coût $J$ décroît ou reste constante. Elle ne peut pas augmenter.
- Si $J$ reste constante pendant une itération, l'algorithme a convergé vers un minimum local.
- Il existe un nombre fini de partitions possibles de $\mathbf{X}$ en $K$ clusters. Donc l'algorithme converge en un nombre fini d'itérations.
- Cependant, l'algorithme peut converger vers différents minima locaux selon l'initialisation des centroïdes.

Pour améliorer les chances de trouver le minimum global, on peut exécuter K-means plusieurs fois avec différentes initialisations aléatoires des centroïdes et garder la meilleure solution.

En résumé, l'objectif d'optimisation de K-means est de minimiser la distorsion moyenne en assignant chaque point au cluster le plus proche et en mettant à jour les centroïdes comme les moyennes des points assignés. Cet objectif est atteint de manière itérative jusqu'à convergence vers un minimum local.

![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/df43b754-8ad1-4265-9772-b8d3485dd5da)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/3c9f6e6a-bd32-4c55-b9b0-65043b66fb25)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/a5eb0ffd-52e9-4e44-b11e-957b96e4287b)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/039fa7bf-f992-411b-82a8-b0550079ad8c)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/3dbcc9c9-0900-4656-a53d-f95d8c7b0074)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/e227c54f-3779-4b40-817b-8101bb0400fd)
![image](https://github.com/hrhouma/Apprentissage-Non-Supervise-1/assets/10111526/787114e7-c9a5-4a64-aff7-5f21056a5c73)





Citations:
- [1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/19265956/915a5827-da73-41f0-99bd-7505326d3a65/paste.txt
- [2] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/19265956/c0944ecf-c4a2-4dff-8ed6-23af32eed583/paste.txt
- [3] https://proceedings.neurips.cc/paper/1994/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf
- [4] https://perso.telecom-paristech.fr/bonald/documents/kmeans.pdf
- [5] https://arxiv.org/pdf/1906.06821.pdf
- [6] https://cseweb.ucsd.edu/~dasgupta/291-unsup/lec2.pdf
- [7] https://www.khoury.northeastern.edu/home/hand/teaching/cs6140-fall-2021/Day-18-K-Means-Clustering.pdf
- [8] https://pubs.aip.org/aip/jcp/article/156/5/054109/2840662/Elucidating-the-solution-structure-of-the-K-means
- [9] https://machinelearningmastery.com/tour-of-optimization-algorithms/
- [10] http://www.cs.yale.edu/homes/el327/datamining2013aFiles/10_k_means_clustering.pdf
- [11] https://www.tutorialspoint.com/how-optimization-in-machine-learning-works
- [12] https://towardsdatascience.com/understanding-optimization-algorithms-in-machine-learning-edfdb4df766b
- [13] https://stats.stackexchange.com/questions/188087/proof-of-convergence-of-k-means
